__author__ = 'Brandon C. Kelly'

import numpy as np
import pymc as pymc
import pytest
from scipy.linalg import cholesky
from scipy import linalg
from my_pymc_steps import RobustAdaptiveMetro
import matplotlib.pyplot as plt
from matplotlib import mlab

# Global parameters: mean and variance for 1-d gaussian data generated by gaussian_data().
true_mean = 2.3
true_variance = 0.45 ** 2

true_mean2d = np.array([1.2, -0.6])
true_covar = np.array([[2.3, -0.7], [-0.7, 1.2]])


def gaussian_data1d():
    """
    Generate some test data drawn from a 1-d Gaussian distribution. This same data will be used for all tests that use
    this fixture function.
    """
    ndata = 1000
    data = np.random.normal(true_mean, np.sqrt(true_variance), ndata)
    return data


def gaussian_data2d():
    """
    Generate some test data drawn from a 2-d Gaussian distribution.
    """
    ndata = 1000
    data = np.random.multivariate_normal(true_mean2d, true_covar, ndata)
    return data

@pymc.stochastic()
def dummy_3dparameter(value=np.zeros(3)):
    """
    This is just an empty stochastic object needed for testing the rank-1 Cholesky decomposition.
    """
    return 0.0


def test_CholUpdateR1():
    """
    Make sure the rank-1 Cholesky update from RobustAdaptiveMetro.CholUpdateR1 agrees with the full Cholesky
    decomposition performed by scipy.linalg.cholesky.
    """
    # Make a positive-definite symmetric array
    corr = np.array([[1.0, 0.3, -0.5], [0.3, 1.0, 0.54], [-0.5, 0.54, 1.0]])
    sigma = np.array([[2.3, 0.0, 0.0], [0.0, 0.45, 0.0], [0.0, 0.0, 13.4]])
    covar = sigma.dot(corr.dot(sigma))

    # Compute the cholesky decomposition: covar = L * L.transpose()
    L = cholesky(covar)

    # Construct the update vector
    z = np.random.standard_normal(3)
    z /= np.linalg.norm(z)
    v = np.sqrt(0.5) * L.transpose().dot(z)

    covar_update = covar + np.outer(v, v)  # Updated covariance matrix
    covar_downdate = covar - np.outer(v, v)  # Downdated covariance matrix

    # Get the cholesky decompositions of the updated and downdated matrices. Do the slow way first.
    Lup0 = cholesky(covar_update)
    Ldown0 = cholesky(covar_downdate)

    # Now get the rank-1 updated and downdated cholesky factors, but do the fast way.
    mcmc = pymc.MCMC({'dummy_3dparameter':dummy_3dparameter})  # The MCMC sampler
    target_rate = 0.4
    mcmc.use_step_method(RobustAdaptiveMetro, dummy_3dparameter, target_rate, proposal_covar=covar)

    # Grab the RAM step object
    RAM = mcmc.step_method_dict[dummy_3dparameter][0]

    vup = v.copy()
    RAM.CholUpdateR1(vup)  # The update/downdate algorithm assumes an upper triangular matrix

    Lup = RAM._cholesky_factor

    RAM = RobustAdaptiveMetro(dummy_3dparameter, target_rate, proposal_covar=covar)
    vdown = v.copy()
    RAM.CholUpdateR1(vdown, downdate=True)

    Ldown = RAM._cholesky_factor

    low_triangle = Lup0 != 0
    frac_diff = np.abs(Lup0[low_triangle] - Lup[low_triangle]) / np.abs(Lup0[low_triangle])
    assert frac_diff.max() < 1e-8

    frac_diff = np.abs(Ldown0[low_triangle] - Ldown[low_triangle]) / np.abs(Ldown0[low_triangle])
    assert frac_diff.max() < 1e-8


def test_RAM1d():
    """
    Test the RobustAdaptiveMetro step from my_pymc_steps.py.

    :param gaussian_data1d: A fixture function that generates some data to use for testing.
    """
    data = gaussian_data1d()

    # Setup PyMC model
    @pymc.stochastic
    def Mu(value=data.mean()):
        """
        Normal parameters (mu,variance).
        """
        return 0.0  # Uniform prior, so log(prior) is just zero.

    @pymc.stochastic(observed=True)
    def normal_data(value=data, Mu=Mu):
        """
        Data generated from a normal distribution.
        """
        data_mean = np.mean(value)
        ndata = value.size
        loglik = -ndata / 2.0 * np.sqrt(2.0 * np.pi * true_variance) - \
            0.5 * ndata * (data_mean - Mu) ** 2 / true_variance
        return loglik

    # Now let's run the MCMC sampler and make sure the RAM algorithm behaves as expected

    mcmc = pymc.MCMC({'Mu':Mu, 'normal_data':normal_data})  # The MCMC sampler

    covar_guess = data.var() / data.size
    target_rate = 0.40
    # Make sure we use the RAM step to update the normal mean.
    mcmc.use_step_method(RobustAdaptiveMetro, Mu, target_rate, proposal_covar=covar_guess, proposal_distribution='T')

    # Before we start the MCMC sampler, let's run some tests to make sure that the RAM step is properly initialized.
    RAM = mcmc.step_method_dict[Mu][0]
    assert RAM._dim == 1
    assert RAM._proposal_distribution == 'T'
    assert np.abs(RAM._cholesky_factor - np.sqrt(data.var() / data.size)) < 1e-5

    niter = 100000
    nburn = 10000

    mcmc.sample(niter, burn=nburn)
    mu_draws = mcmc.trace('Mu')[:]
    assert RAM._current_iter == niter
    assert RAM._accepted + RAM._rejected == niter

    # Make sure acceptance rate is within 2% of the target rate
    acceptance_rate = RAM._accepted / float(RAM._current_iter)
    assert abs(acceptance_rate - target_rate) / abs(target_rate) < 0.02

    # Compare the histogram of the draws obtained from the RAM algorithm
    # with the expected distribution
    xgrid = np.linspace(data.mean() - 5.0 * np.std(data) / np.sqrt(data.size),
                        data.mean() + 5.0 * np.std(data) / np.sqrt(data.size))
    post_var = true_variance / data.size
    post_mean = data.mean()

    post_pdf = 1.0 / np.sqrt(2.0 * np.pi * post_var) * np.exp(-0.5 * (xgrid - post_mean) ** 2 / post_var)

    plt.subplot(111)
    plt.hist(mu_draws, bins=25, normed=True)
    plt.plot(xgrid, post_pdf, 'r', lw=2)
    plt.title("Normal Model: Test of Metropolis step method")
    plt.xlabel("Mean")
    plt.ylabel("PDF")
    plt.show()


def test_RAM2d():

    data = gaussian_data2d()

    # Setup PyMC model
    @pymc.stochastic
    def Mu(value=data.mean(axis=0)):
        """
        Normal parameters (mu,variance).
        """
        return 0.0  # Uniform prior, so log(prior) is just zero.

    @pymc.stochastic(observed=True)
    def normal_data(value=data, Mu=Mu):
        """
        Data generated from a normal distribution.
        """
        data_mean = value.mean(axis=0)
        ndata = value.shape[0]
        zcent = data_mean - Mu
        loglik = -0.5 * ndata * np.dot(zcent.T, np.linalg.inv(true_covar).dot(zcent))
        return loglik

    mcmc = pymc.MCMC({'Mu':Mu, 'normal_data':normal_data})

    covar_guess = np.cov(data, rowvar=0) / data.shape[0]
    target_rate = 0.4
    # Make sure we use the RAM step to update the normal mean.
    mcmc.use_step_method(RobustAdaptiveMetro, Mu, proposal_covar=covar_guess, target_rate=target_rate)

    # Before we start the MCMC sampler, let's run some tests to make sure that the RAM step is properly initialized.
    RAM = mcmc.step_method_dict[Mu][0]
    assert RAM._dim == 2
    assert RAM._proposal_distribution == 'Normal'

    # Now let's run the MCMC sampler and make sure the RAM algorithm behaves as expected
    niter = 100000
    nburn = 50000

    mcmc.sample(niter, nburn)
    assert RAM._current_iter == niter

    mu_draws = mcmc.trace(Mu)[:]

    # Make sure acceptance rate is within 2% of the target rate
    acceptance_rate = RAM._accepted / float(RAM._current_iter)
    assert abs(acceptance_rate - target_rate) / abs(target_rate) < 0.02

    # Compare covariance matrix of proposals with posterior covariance
    post_covar = true_covar / data.shape[0]

    covar_n = RAM._cholesky_factor.T.dot(RAM._cholesky_factor)
    eigenval_n, eigenvect_n = linalg.eig(covar_n)
    eigenval_n = np.diagflat(eigenval_n)
    covroot_n = np.dot(eigenvect_n.T, np.sqrt(eigenval_n).dot(eigenvect_n))  # Matrix square-root of proposal covariance
    covroot_n = covroot_n.real

    eigenval, eigenvect = linalg.eig(post_covar)
    eigenval_inv = np.diagflat(1.0 / eigenval)
    covroot_inv = np.dot(eigenvect.T, np.sqrt(eigenval_inv).dot(eigenvect))
    covroot_inv = covroot_inv.real

    evals, evects = linalg.eig(covroot_n.dot(covroot_inv))
    # Compute the 'suboptimality factor'. This should be unity if the two matrices are proportional.
    subopt_factor = evals.size * np.sum(1.0 / evals ** 2) / (np.sum(1.0 / evals)) ** 2

    assert subopt_factor < 1.01  # Test for proportionality of proposal covariance and posterior covariance

    # Now make nice plots comparing sampled values with the true posterior
    data_mean = data.mean(axis=0)

    xgrid = np.linspace(data_mean[0] - 4.0 * np.sqrt(post_covar[0, 0]),
                        data_mean[0] + 4.0 * np.sqrt(post_covar[0, 0]), 100)
    ygrid = np.linspace(data_mean[1] - 4.0 * np.sqrt(post_covar[1, 1]),
                        data_mean[1] + 4.0 * np.sqrt(post_covar[1, 1]), 100)

    X, Y = np.meshgrid(xgrid, ygrid)

    true_pdf = mlab.bivariate_normal(X, Y, np.sqrt(post_covar[0, 0]), np.sqrt(post_covar[1, 1]),
                                     data_mean[0], data_mean[1], post_covar[0, 1])

    plt.subplot(211)
    post_pdf = 1.0 / np.sqrt(2.0 * np.pi * post_covar[0, 0]) * \
        np.exp(-0.5 * (xgrid - data_mean[0]) ** 2 / post_covar[0, 0])

    plt.hist(mu_draws[:, 0], bins=25, normed=True)
    plt.plot(xgrid, post_pdf, 'r', lw=2)
    plt.title("Bivariate Normal Model: Test of Robust Adaptive Metropolis step method")
    plt.xlabel("Mean 1")
    plt.ylabel("PDF")

    plt.subplot(212)
    post_pdf = 1.0 / np.sqrt(2.0 * np.pi * post_covar[1, 1]) * \
        np.exp(-0.5 * (ygrid - data_mean[1]) ** 2 / post_covar[1, 1])

    plt.hist(mu_draws[:, 1], bins=25, normed=True)
    plt.plot(ygrid, post_pdf, 'r', lw=2)
    plt.xlabel("Mean 2")
    plt.ylabel("PDF")
    plt.show()

    plt.figure()
    plt.plot(mu_draws[:, 0], mu_draws[:, 1], '.', ms=2)
    plt.contour(X, Y, true_pdf, linewidths=5)
    plt.title('Test of Robust Adaptive Metropolis Algorithms: Bivariate Normal Model')
    plt.ylabel('Mean 2')
    plt.xlabel('Mean 1')
    plt.show()

